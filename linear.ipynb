{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOh/5D7SR0bofNHtJoJgkOY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DucBox/Hw_AI/blob/main/linear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jXGYgZKKupj0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy import linalg as LA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class LinearRegression:\n",
        "\n",
        "    def __init__(self, init_theta=None, alpha=0.01, n_iter=100):\n",
        "        '''\n",
        "        Constructor\n",
        "        '''\n",
        "        self.alpha = alpha\n",
        "        self.n_iter = n_iter\n",
        "        self.theta = init_theta\n",
        "        self.JHist = None\n",
        "\n",
        "    def gradientDescent(self, X, y, theta):\n",
        "        '''\n",
        "        Fits the model via gradient descent\n",
        "        Arguments:\n",
        "            X is a n-by-d numpy matrix\n",
        "            y is an n-dimensional numpy vector\n",
        "            theta is a d-dimensional numpy vector\n",
        "        Returns:\n",
        "            the final theta found by gradient descent\n",
        "        '''\n",
        "        n, d = X.shape\n",
        "        self.JHist = []\n",
        "        for i in range(self.n_iter):\n",
        "            self.JHist.append((self.computeCost(X, y, theta), theta))\n",
        "            print(\"Iteration: \", i + 1, \" Cost: \", self.JHist[i][0], \" Theta: \", theta)\n",
        "            # Update theta using gradient descent\n",
        "            theta = theta - (self.alpha / n) * X.T @ (X @ theta - y)\n",
        "\n",
        "        return theta\n",
        "\n",
        "    def computeCost(self, X, y, theta):\n",
        "        '''\n",
        "        Computes the objective function\n",
        "        Arguments:\n",
        "          X is a n-by-d numpy matrix\n",
        "          y is an n-dimensional numpy vector\n",
        "          theta is a d-dimensional numpy vector\n",
        "        Returns:\n",
        "          a scalar value of the cost\n",
        "              ** make certain you don't return a matrix with just one value! **\n",
        "        '''\n",
        "        n = len(y)\n",
        "        cost = (1 / (2 * n)) * np.sum((X @ theta - y) ** 2)\n",
        "        return cost\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        '''\n",
        "        Trains the model\n",
        "        Arguments:\n",
        "            X is a n-by-d numpy matrix\n",
        "            y is an n-dimensional numpy vector\n",
        "        '''\n",
        "        n, d = X.shape\n",
        "        if self.theta is None:\n",
        "            self.theta = np.matrix(np.zeros((d, 1)))\n",
        "        self.theta = self.gradientDescent(X, y, self.theta)\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''\n",
        "        Used the model to predict values for each instance in X\n",
        "        Arguments:\n",
        "            X is a n-by-d numpy matrix\n",
        "        Returns:\n",
        "            an n-dimensional numpy vector of the predictions\n",
        "        '''\n",
        "        predictions = X @ self.theta\n",
        "        return predictions"
      ]
    }
  ]
}