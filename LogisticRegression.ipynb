{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWPRuqiXE2Fj/nszqVxslP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DucBox/Hw_AI/blob/main/LogisticRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yu-rqw5l1IVX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class LogisticRegression:\n",
        "\n",
        "    def __init__(self, alpha=0.01, regLambda=0.01, epsilon=0.0001, maxNumIters=10000):\n",
        "        '''\n",
        "        Constructor\n",
        "        '''\n",
        "        self.alpha = alpha\n",
        "        self.regLambda = regLambda\n",
        "        self.epsilon = epsilon\n",
        "        self.maxNumIters = maxNumIters\n",
        "        self.theta = None\n",
        "\n",
        "    def computeCost(self, theta, X, y, regLambda):\n",
        "        '''\n",
        "        Computes the objective function\n",
        "        Arguments:\n",
        "            X is a n-by-d numpy matrix\n",
        "            y is an n-dimensional numpy vector\n",
        "            regLambda is the scalar regularization constant\n",
        "        Returns:\n",
        "            a scalar value of the cost  ** make certain you're not returning a 1 x 1 matrix! **\n",
        "        '''\n",
        "        m = len(y)\n",
        "        h = self.sigmoid(X @ theta)\n",
        "        J = (-1/m) * (y.T @ np.log(h) + (1 - y).T @ np.log(1 - h)) + (regLambda / (2 * m)) * np.sum(theta[1:]**2)\n",
        "        return J.item()\n",
        "\n",
        "    def computeGradient(self, theta, X, y, regLambda):\n",
        "        '''\n",
        "        Computes the gradient of the objective function\n",
        "        Arguments:\n",
        "            X is a n-by-d numpy matrix\n",
        "            y is an n-dimensional numpy vector\n",
        "            regLambda is the scalar regularization constant\n",
        "        Returns:\n",
        "            the gradient, an d-dimensional vector\n",
        "        '''\n",
        "        m = len(y)\n",
        "        h = self.sigmoid(X @ theta)\n",
        "        grad = (1/m) * X.T @ (h - y) + (regLambda / m) * np.concatenate(([0], theta[1:]))\n",
        "        return grad\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        '''\n",
        "        Trains the model\n",
        "        Arguments:\n",
        "            X is a n-by-d numpy matrix\n",
        "            y is an n-dimensional numpy vector\n",
        "        '''\n",
        "        n, d = X.shape\n",
        "        X = np.concatenate((np.ones((n, 1)), X), axis=1)  # Add a column of ones to X for the bias term\n",
        "        self.theta = np.zeros((d + 1, 1))\n",
        "\n",
        "        for _ in range(self.maxNumIters):\n",
        "            gradient = self.computeGradient(self.theta, X, y, self.regLambda)\n",
        "            self.theta -= self.alpha * gradient\n",
        "\n",
        "            if np.linalg.norm(gradient) < self.epsilon:\n",
        "                break\n",
        "\n",
        "    def predict(self, X):\n",
        "        '''\n",
        "        Used the model to predict values for each instance in X\n",
        "        Arguments:\n",
        "            X is a n-by-d numpy matrix\n",
        "        Returns:\n",
        "            an n-dimensional numpy vector of the predictions\n",
        "        '''\n",
        "        n = X.shape[0]\n",
        "        X = np.concatenate((np.ones((n, 1)), X), axis=1)  # Add a column of ones to X for the bias term\n",
        "        predictions = np.round(self.sigmoid(X @ self.theta))\n",
        "        return predictions.flatten().astype(int)\n",
        "\n",
        "    def sigmoid(self, Z):\n",
        "        '''\n",
        "        Computes the sigmoid function 1/(1+exp(-z))\n",
        "        '''\n",
        "        return 1 / (1 + np.exp(-Z))\n"
      ]
    }
  ]
}